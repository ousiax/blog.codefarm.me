<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bing WebMaster -->
  <meta name="msvalidate.01" content="AB2FFF876C37F59D9121882CC8395DE5" />

  <title>RAG: Boosting LLMs with Contextual Retrieval</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://blog.codefarm.me/2024/06/18/rag-boosting-llms-with-contextual-retrieval/">
  <link rel="alternate" type="application/rss+xml" title="CODE FARM" href="https://blog.codefarm.me/feed.xml">

  <!--<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" />-->

  <!-- https://cdn.jsdelivr.net/gh/lurongkai/anti-baidu/js/anti-baidu-latest.min.js -->
<script type="text/javascript" src="/js/anti-baidu.min.js" charset="UTF-8"></script>

  
<!-- Google Analytics Website tracking -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83971182-1', 'auto');
  ga('send', 'pageview');

</script>


  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SN88FJ18E5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SN88FJ18E5');
</script>



</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    <h2 class="site-title">
      <a class="site-title" href="/">CODE FARM</a>
    </h2>

     <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
        <div class="trigger">
            <ul>
                <li><a href="/">home</a>
                <li><a href="/category">category</a>
                <li><a href="/tag">tag</a>
                <li><a href="/archive">archive</a>
                <li><a href="/about">about</a>
                <li><a href="https://resume.github.io/?ousiax" target="_blank">R&eacute;sum&eacute;</a>
            </ul>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">RAG: Boosting LLMs with Contextual Retrieval</h1>
    
    
    <p class="post-meta"><time datetime="2024-06-18T13:33:05+08:00" itemprop="datePublished">Jun 18, 2024</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>RAG (Retrieval-Augmented Generation) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) like GPT-4. While LLMs excel at generating text, they often lack context and struggle to understand the deeper meaning behind user queries. RAG bridges this gap by incorporating information retrieval to provide LLMs with relevant context, leading to improved response quality.</p>
</div>
</div>
<div id="toc" class="toc">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#how-does-rag-work">1. How does RAG work?</a></li>
<li><a href="#deep-dive-into-context-enrichment-for-rag-systems">2. Deep Dive into Context Enrichment for RAG Systems</a></li>
<li><a href="#automatic-prompt-construction">3. Automatic Prompt Construction</a></li>
<li><a href="#build-rag-with-milvus">4. Build RAG with Milvus</a>
<ul class="sectlevel2">
<li><a href="#prepare-the-data-in-milvus">4.1. Prepare the data in Milvus</a></li>
<li><a href="#use-llm-to-get-a-rag-response">4.2. Use LLM to get a RAG response</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>
</div>
<div class="sect1">
<h2 id="how-does-rag-work">1. How does RAG work?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>RAG is a pattern which uses your data with an LLM to generate answers specific to your data. When a user asks a question, the data store is searched based on user input. The user question is then combined with the matching results and sent to the LLM using a prompt (explicit instructions to an AI or machine learning model) to generate the desired answer. This can be illustrated as follows. <a href="#ms-az-ai-rag">[1]</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://learn.microsoft.com/en-us/azure/ai-studio/media/index-retrieve/rag-pattern.png#lightbox" alt="Screenshot of the RAG pattern." width="55%" height="55%">
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>User Input: The user provides a query or prompt.</p>
</li>
<li>
<p>Vector Search: A vector database (like Milvus) efficiently retrieves documents or passages most relevant to the user&#8217;s query based on semantic similarity.</p>
</li>
<li>
<p>Context Enrichment: Techniques like summarization, keyphrase extraction, or entity recognition are applied to the retrieved information, providing context for the LLM.</p>
</li>
<li>
<p>Prompt Construction: The user&#8217;s original query is combined with the extracted context to form a new, enriched prompt for the LLM.</p>
</li>
<li>
<p>Enhanced Generation: The LLM leverages the enriched prompt to generate a more informative and relevant response that addresses the user&#8217;s specific intent and considers the retrieved context.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>While Milvus and GPT-like LLMs are key players, consider these additional aspects for a well-rounded RAG system:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Machine Learning Fundamentals: Understanding concepts like word embeddings and information retrieval is crucial.</p>
</li>
<li>
<p>Alternative Tools: Explore other vector databases and pre-trained word embedding models.</p>
</li>
<li>
<p>Prompt Construction Techniques: Utilize template-based prompts, conditional logic, or fine-tuning for automatic prompt generation.</p>
</li>
<li>
<p>Evaluation: Continuously monitor performance to identify areas for improvement.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, RAG empowers LLMs to become more contextually aware, leading to a more informative and engaging user experience.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="deep-dive-into-context-enrichment-for-rag-systems">2. Deep Dive into Context Enrichment for RAG Systems</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Context enrichment is a crucial step in RAG (Retrieval-Augmented Generation) that bridges the gap between a user&#8217;s query and the LLM&#8217;s response. It involves processing the information retrieved from the vector database (like Milvus) to provide the LLM with a deeper understanding of the user&#8217;s intent and the relevant context.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown of some popular libraries and techniques for context enrichment:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Text Summarization:</p>
<div class="ulist">
<ul>
<li>
<p>Goal: Condense retrieved documents into concise summaries for the LLM to grasp the key points.</p>
</li>
<li>
<p>Libraries:</p>
<div class="ulist">
<ul>
<li>
<p>Gensim (Python): Offers various summarization techniques, including extractive (selecting important sentences) and abstractive (generating a new summary).</p>
</li>
<li>
<p>BART (Transformers library): A powerful pre-trained model specifically designed for text summarization.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Keyword Extraction:</p>
<div class="ulist">
<ul>
<li>
<p>Goal: Identify the most relevant keywords or keyphrases within retrieved documents to highlight the main themes.</p>
</li>
<li>
<p>Libraries:</p>
<div class="ulist">
<ul>
<li>
<p>spaCy (Python): Provides functionalities for part-of-speech tagging, named entity recognition, and keyword extraction.</p>
</li>
<li>
<p>NLTK (Python): A comprehensive toolkit for various NLP tasks, including keyword extraction using techniques like TF-IDF (Term Frequency-Inverse Document Frequency).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Named Entity Recognition (NER):</p>
<div class="ulist">
<ul>
<li>
<p>Goal: Recognize and classify named entities (people, locations, organizations) within retrieved text, enriching the context for the LLM.</p>
</li>
<li>
<p>Libraries:</p>
<div class="ulist">
<ul>
<li>
<p>spaCy: Offers pre-trained NER models for various languages, allowing the LLM to understand the context of specific entities.</p>
</li>
<li>
<p>Stanford NER: A widely used Java-based library for named entity recognition.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Choosing the Right Technique:</strong></p>
</div>
<div class="paragraph">
<p>The best approach for context enrichment depends on your specific needs and the type of data you&#8217;re working with. Here&#8217;s a quick guide:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For factual or informative responses: Text summarization can be highly effective.</p>
</li>
<li>
<p>For understanding the main topics: Keyword extraction is a good choice.</p>
</li>
<li>
<p>For tasks involving specific entities: Named entity recognition becomes crucial.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Advanced Techniques:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Combining Techniques: Don&#8217;t be limited to a single approach. Combine summarization with keyword extraction or NER to provide richer context to the LLM.</p>
</li>
<li>
<p>Custom Summarization Models: For specialized domains, consider training custom summarization models using domain-specific data.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="automatic-prompt-construction">3. Automatic Prompt Construction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Several approaches can automate prompt construction based on user input and extracted context:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Template-Based Prompts: Pre-defined templates can be used to structure the prompt, incorporating user query and extracted elements (e.g., "{user_query}: Based on similar content, here are some key points: {key_phrases}. Can you elaborate?").</p>
</li>
<li>
<p>Conditional Logic: Conditional statements can be used based on the chosen context enrichment technique. For example, if using summaries, the prompt might say "Here&#8217;s a summary of relevant information&#8230;&#8203;" while using keyphrases, it might mention "Here are some key points&#8230;&#8203;"</p>
</li>
<li>
<p>Fine-tuning Language Models: Techniques like fine-tuning pre-trained LLMs can be explored to allow them to automatically learn how to integrate user queries and retrieved context into a cohesive prompt. This is an advanced approach requiring expertise in machine learning.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Choosing the Right Tool:</strong></p>
</div>
<div class="paragraph">
<p>The best tool or approach depends on your specific needs and available resources. Here&#8217;s a basic guideline:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Simpler Systems: For less complex RAG systems, template-based prompts with basic summarization or keyword extraction tools might suffice.</p>
</li>
<li>
<p>Advanced Systems: For more sophisticated applications, consider exploring conditional logic, fine-tuning LLMs, or combining different context enrichment techniques to create richer prompts.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>By combining vector databases with the right context enrichment tools and automatic prompt construction techniques, we can build a robust RAG system that leverages the power of LLMs to generate more informative and relevant responses.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="build-rag-with-milvus">4. Build RAG with Milvus</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will use <a href="https://azure.microsoft.com/en-us/products/phi-3">Phi-3</a>, an open small language model, to provide an OpenAI-compatible API.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Prepare the Phi3 LLM with Ollama on Linux</div>
<div class="ulist">
<ul>
<li>
<p>Install Ollama on Linux:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">curl <span class="nt">-fsSL</span> https://ollama.com/install.sh | sh</code></pre>
</div>
</div>
</li>
<li>
<p>Pull model <code>phi3:mini</code>, and make sure the model checkpoint is prepared:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">ollama pull phi3:mini</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>ollama list
<span class="go">NAME                    ID              SIZE    MODIFIED
phi3:mini               64c1188f2485    2.4 GB  17 minutes ago</span></code></pre>
</div>
</div>
</li>
<li>
<p>Check the Phi3 model with the Chat Completion API:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">curl http://localhost:11434/v1/chat/completions <span class="se">\</span>
    <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
    <span class="nt">-d</span> <span class="s1">'{"model":"phi3:mini","messages":[{"role":"user","content":"Hi, who are you?"}]}'</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{
  "id": "chatcmpl-866",
  "object": "chat.completion",
  "created": 1718872510,
  "model": "phi3:mini",
  "system_fingerprint": "fp_ollama",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": " I am Phi, an AI developed to provide information and answer questions to the best of my ability. How can I assist you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 30,
    "total_tokens": 30
  }
}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="prepare-the-data-in-milvus">4.1. Prepare the data in Milvus</h3>
<div class="ulist">
<ul>
<li>
<p>Dependencies and Environment</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">pip <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="s1">'pymilvus[model]==2.4.4'</span> <span class="s1">'numpy&lt;2'</span> openai requests
<span class="c"># pipenv install -v 'pymilvus[model]==2.4.4' 'numpy&lt;2'  openai requests</span></code></pre>
</div>
</div>
</li>
<li>
<p>Prepare the embedding model</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus.model.dense</span> <span class="kn">import</span> <span class="n">SentenceTransformerEmbeddingFunction</span>  <span class="c1"># Sentence Transformer pre-trained models
</span>
<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="n">ef</span> <span class="o">=</span> <span class="nc">SentenceTransformerEmbeddingFunction</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># Specify the model name
</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create the collection in Milvus</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span><span class="p">,</span> <span class="n">DataType</span>

<span class="n">COLLECTION_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">my_rag_collection</span><span class="sh">"</span>
<span class="n">SERVER_ADDR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span>
<span class="n">ACCESS_TOKEN</span> <span class="o">=</span> <span class="sh">"</span><span class="s">root:Milvus</span><span class="sh">"</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">default</span><span class="sh">"</span>

<span class="c1"># 1. Set up a Milvus client
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="n">SERVER_ADDR</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">ACCESS_TOKEN</span><span class="p">,</span>
    <span class="n">db_name</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 2. Check if the collection already exists and drop it if it does.
</span><span class="k">if</span> <span class="n">client</span><span class="p">.</span><span class="nf">has_collection</span><span class="p">(</span><span class="n">COLLECTION_NAME</span><span class="p">):</span>
    <span class="n">client</span><span class="p">.</span><span class="nf">drop_collection</span><span class="p">(</span><span class="n">COLLECTION_NAME</span><span class="p">)</span>

<span class="c1"># 3. Create a new collection with specified parameters.
</span><span class="n">client</span><span class="p">.</span><span class="nf">create_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
    <span class="n">dimension</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span>  <span class="c1"># The vector has 384 dimensions, matching the SBERT embedding function with all-MiniLM-L6-v2
</span>    <span class="n">auto_id</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># default is False
</span>    <span class="c1"># primary_field_name="id",
</span>    <span class="c1"># id_type="int",
</span>    <span class="c1"># vector_field_name="vector",
</span>    <span class="c1"># metric_type="COSINE",
</span>    <span class="c1"># enable_dynamic_field=True,
</span><span class="p">)</span>

<span class="c1"># 4. (optional) To load a collection, use the load_collection() method.
# client.load_collection(
#     collection_name=COLLECTION_NAME
# )
#
# To release a collection, use the release_collection() method.
# client.release_collection(
#     collection_name=COLLECTION_NAME
# )
</span>
<span class="c1"># 5. (optional) The collection created above is loaded automatically.
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_load_state</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># 6. (optional) List detailed information about the collection.
</span><span class="kn">import</span> <span class="n">json</span>
<span class="n">desc</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">describe_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">desc</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">{'state': &lt;LoadState: Loaded&gt;</span><span class="o">}</span>
<span class="go">{
  "collection_name": "my_rag_collection",
  "auto_id": true,
  "num_shards": 1,
  "description": "",
  "fields": [
    {
      "field_id": 100,
      "name": "id",
      "description": "",
      "type": 5,
      "params": {},
      "auto_id": true,
      "is_primary": true
    },
    {
      "field_id": 101,
      "name": "vector",
      "description": "",
      "type": 101,
      "params": {
        "dim": 384
      }
    }
  ],
  "aliases": [],
  "collection_id": 450568843972908135,
  "consistency_level": 2,
  "properties": {},
  "num_partitions": 1,
  "enable_dynamic_field": true
}</span></code></pre>
</div>
</div>
</li>
<li>
<p>Use the <a href="https://github.com/milvus-io/milvus/blob/master/DEVELOPMENT.md">Milvus development guide</a> to be as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># download and save it as a local text file.
</span><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">urllib.request</span>

<span class="n">URL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md</span><span class="sh">"</span>
<span class="n">FILE_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./Milvus_DEVELOPMENT.md1</span><span class="sh">"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">FILE_PATH</span><span class="p">):</span>
    <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="nf">urlretrieve</span><span class="p">(</span><span class="n">URL</span><span class="p">,</span> <span class="n">FILE_PATH</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create embeddings, and then insert the data into Milvus</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span><span class="p">,</span> <span class="n">model</span>

<span class="n">COLLECTION_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">my_rag_collection</span><span class="sh">"</span>
<span class="n">SERVER_ADDR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span>
<span class="n">ACCESS_TOKEN</span> <span class="o">=</span> <span class="sh">"</span><span class="s">root:Milvus</span><span class="sh">"</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">default</span><span class="sh">"</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="n">SERVER_ADDR</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">ACCESS_TOKEN</span><span class="p">,</span>
    <span class="n">db_name</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="n">ef</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">dense</span><span class="p">.</span><span class="nc">SentenceTransformerEmbeddingFunction</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">FILE_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./Milvus_DEVELOPMENT.md</span><span class="sh">"</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">FILE_PATH</span><span class="p">,</span> <span class="sh">"</span><span class="s">r+t</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fi</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">fi</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="c1"># Use "# " to separate the content in the file, which can roughly separate
# the content of each main part of the markdown file.
</span><span class="n">docs</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"># </span><span class="sh">"</span><span class="p">)</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">ef</span><span class="p">.</span><span class="nf">encode_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">:</span> <span class="n">vectors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">docs</span><span class="p">[</span><span class="n">i</span><span class="p">]}</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">vectors</span><span class="p">))]</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'insert_count': 47, 'ids': [450568843971283844, ... , 450568843971283889, 450568843971283890], 'cost': 0}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="use-llm-to-get-a-rag-response">4.2. Use LLM to get a RAG response</h3>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span><span class="p">,</span> <span class="n">model</span>

<span class="n">COLLECTION_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">my_rag_collection</span><span class="sh">"</span>
<span class="n">SERVER_ADDR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span>
<span class="n">ACCESS_TOKEN</span> <span class="o">=</span> <span class="sh">"</span><span class="s">root:Milvus</span><span class="sh">"</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">default</span><span class="sh">"</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="n">SERVER_ADDR</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">ACCESS_TOKEN</span><span class="p">,</span>
    <span class="n">db_name</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="n">ef</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">dense</span><span class="p">.</span><span class="nc">SentenceTransformerEmbeddingFunction</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define a query question about the content of the development guide documentation.
</span><span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">what is the hardware requirements specification if I want to build Milvus and run from source code?</span><span class="sh">"</span>

<span class="c1"># Search for the question in the collection and retrieve the semantic top-3 matches.
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">ef</span><span class="p">.</span><span class="nf">encode_queries</span><span class="p">([</span><span class="n">question</span><span class="p">]),</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># Return top 3 results
</span>    <span class="n">output_fields</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span>  <span class="c1"># Return the text field
</span><span class="p">)</span>

<span class="n">retrieved_lines_with_distances</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="sh">"</span><span class="s">entity</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="sh">"</span><span class="s">distance</span><span class="sh">"</span><span class="p">])</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">]</span>
<span class="c1"># [
#   [
#     "Hardware Requirements\n\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\n\n```\n- 8GB of RAM\n- 50GB of free disk space\n```\n\n##",
#     0.8904632329940796
#   ],
#   [
#     "Software Requirements\n\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\n\nHere's a list of verified OS types where Milvus can successfully build and run:\n\n- Debian/Ubuntu\n- Amazon Linux\n- MacOS (x86_64)\n- MacOS (Apple Silicon)\n\n##",
#     0.7089803814888
#   ],
#   [
#     "Building Milvus on a local OS/shell environment\n\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\n\n##",
#     0.7013456225395203
#   ]
# ]
</span>
<span class="c1"># Convert the retrieved documents into a string format.
</span><span class="n">context</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span>
    <span class="p">[</span><span class="n">line_with_distance</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">line_with_distance</span> <span class="ow">in</span> <span class="n">retrieved_lines_with_distances</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Define system and user prompts for the Lanage Model.
</span><span class="n">SYSTEM_PROMPT</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.
</span><span class="sh">"""</span>
<span class="n">USER_PROMPT</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
Use the following pieces of information enclosed in &lt;context&gt; tags to provide an answer to the question enclosed in &lt;question&gt; tags.
&lt;context&gt;
</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">
&lt;/context&gt;
&lt;question&gt;
</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s">
&lt;/question&gt;
</span><span class="sh">"""</span>

<span class="c1"># Use OpenAI Chat Completion API to generate a response based on the prompts.
</span><span class="n">OAI_API_KEY</span> <span class="o">=</span> <span class="sh">"</span><span class="s">EMPTY</span><span class="sh">"</span>
<span class="n">OAI_API_BASE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:11434/v1</span><span class="sh">"</span>

<span class="n">oai_client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">OAI_API_KEY</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="n">OAI_API_BASE</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">oai_client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">phi3:mini</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">SYSTEM_PROMPT</span><span class="p">},</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">USER_PROMPT</span><span class="p">},</span>
    <span class="p">],</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">The hardware requirements specification for building Milvus and running it from source code includes having at least 8GB of RAM and 50GB of free disk space.</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="references">References</h2>
<div class="sectionbody">
<div class="ulist bibliography">
<ul class="bibliography">
<li>
<p><a id="ms-az-ai-rag"></a>[1] <a href="https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation" class="bare">https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation</a></p>
</li>
<li>
<p><a id="milvus-arg"></a>[2 ] <a href="https://milvus.io/docs/build-rag-with-milvus.md" class="bare">https://milvus.io/docs/build-rag-with-milvus.md</a></p>
</li>
</ul>
</div>
</div>
</div>
    
<style>
  .utterances {
      max-width: 100%;
  }
</style>
<script src="https://utteranc.es/client.js"
        repo="ousiax/utterances"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>


  </div>

  <ul class="post-navigation">
    <li>
      
      <a href="/2024/06/15/proxies-in-docker-containerd/">&laquo; Proxies in Docker and containerd</a>
      
    </li>
    <li>
      
      <a href="/2024/06/19/dimensions-embedding-models/">Dimensions and Embedding Models &raquo;</a>
      
    </li>
  </ul>
</article>

      </div>
    </div>

    <footer class="site-footer">
  <div class="license">
    <span>Article licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span>
  </div>
  
  <details open>
    <summary>Extral Links</summary>
    <div>
      
      <a href="https://jekyllrb.com/">Jekyll</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://shopify.github.io/">Liquid</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://docs.asciidoctor.org/">Asciidoctor</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://github.com/qqbuby/">GitHub</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="/feed.xml">RSS</a>
      
      
    </div>
  </details>
  
</footer>


<!-- https://github.com/bryanbraun/anchorjs -->
<script src="/js/anchor.min.js"></script>
<script>
  anchors.add();
  anchors.remove(".site-title");
</script>




  </body>

</html>
